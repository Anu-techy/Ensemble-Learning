**Ensemble learning** combines multiple models to produce a stronger, more robust model.

The idea is: a group of weak models can perform better than one strong model.

There are three main **types of ensemble learning:**

        Bagging (e.g., Random Forest)

        Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost)

        Stacking (meta-model on top of multiple models)

**Why Use Ensemble Learning?**

Individual models may make mistakes or overfit.

Combining them reduces bias, variance, or both.

The ensemble "votes" or averages out to give a more reliable result.

**Real-World Analogy**

Think of ensemble learning like getting multiple expert opinions before making a decision. One expert might miss something, but a group is more likely to get it right.
----------------------------------------

Think of Bagging as: same model, different data

Think of Voting as: different models, same data

